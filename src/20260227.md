# 20260227
### 1. iflow cli
Install via:      

Failed:   

```
$ sudo apt install -y npm
$ sudo npm i -g @iflow-ai/iflow-cli 
```

Install via:      

```
bash -c "$(curl -fsSL https://gitee.com/iflow-ai/iflow-cli/raw/main/install.sh)"
```
### 2. Qwen3 
Install nvcc:     

```
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb                                                                             
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update     
sudo apt install -y cuda-toolkit-13-1
export PATH=/usr/local/cuda-13.1/bin${PATH:+:${PATH}}                                   
export LD_LIBRARY_PATH=/usr/local/cuda-13.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}                                                                                               
nvcc --version
echo 'export PATH=/usr/local/cuda-13.1/bin${PATH:+:${PATH}}' >> ~/.bashrc               
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-13.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bash                                                                             
source ~/.bashrc
```
Build llama-cli:     

```
git clone https://github.com/ggerganov/llama.cpp.git --depth 1                          
cd llama.cpp                                                                            
rm -rf build
CMAKE_ARGS="-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=86" cmake -B build -DCMAKE_BUILD_TYPE=Release -G Ninja   -DCMAKE_CUDA_COMPILER=/usr/local/cuda-13.1/bin/nvcc
cmake --build build --config Release -j$(nproc)
./build/bin/llama-cli --version
```
Test the cuda:      

```
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct-GGUF Qwen2.5-0.5B-Instruct-Q8_0.gguf   --local-dir ~/models/test   --local-dir-use-symlinks False
./build/bin/llama-cli \
  -m ~/models/test/qwen2.5-0.5b-instruct-q8_0.gguf \
  -p "Hello" -n 32 \
  --n-gpu-layers -1 \
  --verbose 2>&1 | grep -i "cuda\|ggml_cuda\|gpu\|offload\|cublas"
```

Fetch the model:    

```
pip install -U huggingface_hub[cli] --break-system-packages
export HF_ENDPOINT=https://hf-mirror.com
hf download unsloth/Qwen3.5-35B-A3B-GGUF Qwen3.5-35B-A3B-Q8_0.gguf --local-dir ./models/Qwen3.5-35B-A3B-GGUF
```
Usage in opencode:      

```
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "my-llama-server": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Qwen3.5-35B local (A6000)",
      "options": {
        "baseURL": "http://192.168.1.100:8080/v1",
        "apiKey": "lm-studio"   // 可以随便填，llama-server 不校验
      },
      "models": {
        "Qwen3.5-35B-A3B-Q8_0": {
          "name": "Qwen3.5-35B-A3B Q8_0 (local)",
          "limit": {
            "context": 131072,
            "output": 65536
          }
        }
      }
    }
  }
}
```
